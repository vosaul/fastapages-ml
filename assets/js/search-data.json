{
  
    
        "post0": {
            "title": "1. From epidemic to pandemic",
            "content": "library(readr) library(ggplot2) library(dplyr) # Read datasets/confirmed_cases_worldwide.csv into confirmed_cases_worldwide confirmed_cases_worldwide &lt;- read_csv(&quot;datasets/confirmed_cases_worldwide.csv&quot;) # Print out confirmed_cases_worldwide confirmed_cases_worldwide . ── Column specification ──────────────────────────────────────────────────────── cols( date = col_date(format = &#34;&#34;), cum_cases = col_double() ) . A spec_tbl_df: 56 × 2 datecum_cases . &lt;date&gt;&lt;dbl&gt; . 2020-01-22 | 555 | . 2020-01-23 | 653 | . 2020-01-24 | 941 | . 2020-01-25 | 1434 | . 2020-01-26 | 2118 | . 2020-01-27 | 2927 | . 2020-01-28 | 5578 | . 2020-01-29 | 6166 | . 2020-01-30 | 8234 | . 2020-01-31 | 9927 | . 2020-02-01 | 12038 | . 2020-02-02 | 16787 | . 2020-02-03 | 19881 | . 2020-02-04 | 23892 | . 2020-02-05 | 27635 | . 2020-02-06 | 30817 | . 2020-02-07 | 34391 | . 2020-02-08 | 37120 | . 2020-02-09 | 40150 | . 2020-02-10 | 42762 | . 2020-02-11 | 44802 | . 2020-02-12 | 45221 | . 2020-02-13 | 60368 | . 2020-02-14 | 66885 | . 2020-02-15 | 69030 | . 2020-02-16 | 71224 | . 2020-02-17 | 73258 | . 2020-02-18 | 75136 | . 2020-02-19 | 75639 | . 2020-02-20 | 76197 | . 2020-02-21 | 76823 | . 2020-02-22 | 78579 | . 2020-02-23 | 78965 | . 2020-02-24 | 79568 | . 2020-02-25 | 80413 | . 2020-02-26 | 81395 | . 2020-02-27 | 82754 | . 2020-02-28 | 84120 | . 2020-02-29 | 86011 | . 2020-03-01 | 88369 | . 2020-03-02 | 90306 | . 2020-03-03 | 92840 | . 2020-03-04 | 95120 | . 2020-03-05 | 97882 | . 2020-03-06 | 101784 | . 2020-03-07 | 105821 | . 2020-03-08 | 109795 | . 2020-03-09 | 113561 | . 2020-03-10 | 118592 | . 2020-03-11 | 125865 | . 2020-03-12 | 128343 | . 2020-03-13 | 145193 | . 2020-03-14 | 156097 | . 2020-03-15 | 167449 | . 2020-03-16 | 181531 | . 2020-03-17 | 197146 | . 2. Confirmed cases throughout the world . The table above shows the cumulative confirmed cases of COVID-19 worldwide by date. Just reading numbers in a table makes it hard to get a sense of the scale and growth of the outbreak. Let&#39;s draw a line plot to visualize the confirmed cases worldwide. . # Label the y-axis ggplot(confirmed_cases_worldwide, aes(date, cum_cases)) + geom_line() + ylab(&quot;Cumulative confirmed cases&quot;) . 3. China compared to the rest of the world . The y-axis in that plot is pretty scary, with the total number of confirmed cases around the world approaching 200,000. Beyond that, some weird things are happening: there is an odd jump in mid February, then the rate of new cases slows down for a while, then speeds up again in March. We need to dig deeper to see what is happening. . Early on in the outbreak, the COVID-19 cases were primarily centered in China. Let&#39;s plot confirmed COVID-19 cases in China and the rest of the world separately to see if it gives us any insight. . We&#39;ll build on this plot in future tasks. One thing that will be important for the following tasks is that you add aesthetics within the line geometry of your ggplot, rather than making them global aesthetics. . confirmed_cases_china_vs_world &lt;- read_csv(&quot;datasets/confirmed_cases_china_vs_world.csv&quot;) # See the result glimpse(confirmed_cases_china_vs_world) # Draw a line plot of cumulative cases vs. date, colored by is_china # Define aesthetics within the line geom plt_cum_confirmed_cases_china_vs_world &lt;- ggplot(confirmed_cases_china_vs_world) + geom_line(aes(date, cum_cases, color = is_china)) + ylab(&quot;Cumulative confirmed cases&quot;) # See the plot plt_cum_confirmed_cases_china_vs_world . ── Column specification ──────────────────────────────────────────────────────── cols( is_china = col_character(), date = col_date(format = &#34;&#34;), cases = col_double(), cum_cases = col_double() ) . Rows: 112 Columns: 4 $ is_china &lt;chr&gt; &#34;China&#34;, &#34;China&#34;, &#34;China&#34;, &#34;China&#34;, &#34;China&#34;, &#34;China&#34;, &#34;China… $ date &lt;date&gt; 2020-01-22, 2020-01-23, 2020-01-24, 2020-01-25, 2020-01-26,… $ cases &lt;dbl&gt; 548, 95, 277, 486, 669, 802, 2632, 578, 2054, 1661, 2089, 47… $ cum_cases &lt;dbl&gt; 548, 643, 920, 1406, 2075, 2877, 5509, 6087, 8141, 9802, 118… . 4. Let&#39;s annotate! . Wow! The two lines have very different shapes. In February, the majority of cases were in China. That changed in March when it really became a global outbreak: around March 14, the total number of cases outside China overtook the cases inside China. This was days after the WHO declared a pandemic. . There were a couple of other landmark events that happened during the outbreak. For example, the huge jump in the China line on February 13, 2020 wasn&#39;t just a bad day regarding the outbreak; China changed the way it reported figures on that day (CT scans were accepted as evidence for COVID-19, rather than only lab tests). . By annotating events like this, we can better interpret changes in the plot. . who_events &lt;- tribble( ~ date, ~ event, &quot;2020-01-30&quot;, &quot;Global health nemergency declared&quot;, &quot;2020-03-11&quot;, &quot;Pandemic ndeclared&quot;, &quot;2020-02-13&quot;, &quot;China reporting nchange&quot; ) %&gt;% mutate(date = as.Date(date)) # Using who_events, add vertical dashed lines with an xintercept at date # and text at date, labeled by event, and at 100000 on the y-axis plt_cum_confirmed_cases_china_vs_world + geom_vline(aes(xintercept = date), data = who_events, linetype = &quot;dashed&quot;) + geom_text(aes(date, label = event), data = who_events, y = 1e5) . 5. Adding a trend line to China . When trying to assess how big future problems are going to be, we need a measure of how fast the number of cases is growing. A good starting point is to see if the cases are growing faster or slower than linearly. . There is a clear surge of cases around February 13, 2020, with the reporting change in China. However, a couple of days after, the growth of cases in China slows down. How can we describe COVID-19&#39;s growth in China after February 15, 2020? . china_after_feb15 &lt;- confirmed_cases_china_vs_world %&gt;% filter(is_china == &quot;China&quot;, date &gt;= &quot;2020-02-15&quot;) # Using china_after_feb15, draw a line plot cum_cases vs. date # Add a smooth trend line using linear regression, no error bars ggplot(china_after_feb15, aes(date, cum_cases)) + geom_line() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ylab(&quot;Cumulative confirmed cases&quot;) . `geom_smooth()` using formula &#39;y ~ x&#39; . 6. And the rest of the world? . From the plot above, the growth rate in China is slower than linear. That&#39;s great news because it indicates China has at least somewhat contained the virus in late February and early March. . How does the rest of the world compare to linear growth? . not_china &lt;- confirmed_cases_china_vs_world %&gt;% filter(is_china == &quot;Not China&quot;) # Using not_china, draw a line plot cum_cases vs. date # Add a smooth trend line using linear regression, no error bars plt_not_china_trend_lin &lt;- ggplot(not_china, aes(date, cum_cases)) + geom_line() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ylab(&quot;Cumulative confirmed cases&quot;) # See the result plt_not_china_trend_lin . `geom_smooth()` using formula &#39;y ~ x&#39; . 7. Adding a logarithmic scale . From the plot above, we can see a straight line does not fit well at all, and the rest of the world is growing much faster than linearly. What if we added a logarithmic scale to the y-axis? . plt_not_china_trend_lin + scale_y_log10() . `geom_smooth()` using formula &#39;y ~ x&#39; . 8. Which countries outside of China have been hit hardest? . With the logarithmic scale, we get a much closer fit to the data. From a data science point of view, a good fit is great news. Unfortunately, from a public health point of view, that means that cases of COVID-19 in the rest of the world are growing at an exponential rate, which is terrible news. . Not all countries are being affected by COVID-19 equally, and it would be helpful to know where in the world the problems are greatest. Let&#39;s find the countries outside of China with the most confirmed cases in our dataset. . confirmed_cases_by_country &lt;- read_csv(&quot;datasets/confirmed_cases_by_country.csv&quot;) glimpse(confirmed_cases_by_country) # Group by country, summarize to calculate total cases, find the top 7 top_countries_by_total_cases &lt;- confirmed_cases_by_country %&gt;% group_by(country) %&gt;% summarize(total_cases = max(cum_cases)) %&gt;% top_n(7, total_cases) # See the result top_countries_by_total_cases . ── Column specification ──────────────────────────────────────────────────────── cols( country = col_character(), province = col_character(), date = col_date(format = &#34;&#34;), cases = col_double(), cum_cases = col_double() ) . Rows: 13,272 Columns: 5 $ country &lt;chr&gt; &#34;Afghanistan&#34;, &#34;Albania&#34;, &#34;Algeria&#34;, &#34;Andorra&#34;, &#34;Antigua and… $ province &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … $ date &lt;date&gt; 2020-01-22, 2020-01-22, 2020-01-22, 2020-01-22, 2020-01-22,… $ cases &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … $ cum_cases &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … . A tibble: 7 × 2 countrytotal_cases . &lt;chr&gt;&lt;dbl&gt; . France | 7699 | . Germany | 9257 | . Iran | 16169 | . Italy | 31506 | . Korea, South | 8320 | . Spain | 11748 | . US | 6421 | . 9. Plotting hardest hit countries as of Mid-March 2020 . Even though the outbreak was first identified in China, there is only one country from East Asia (South Korea) in the above table. Four of the listed countries (France, Germany, Italy, and Spain) are in Europe and share borders. To get more context, we can plot these countries&#39; confirmed cases over time. . Finally, congratulations on getting to the last step! If you would like to continue making visualizations or find the hardest hit countries as of today, you can do your own analyses with the latest data available here. . confirmed_cases_top7_outside_china &lt;- read_csv(&quot;datasets/confirmed_cases_top7_outside_china.csv&quot;) # Glimpse at the contents of confirmed_cases_top7_outside_china glimpse(confirmed_cases_top7_outside_china) # Using confirmed_cases_top7_outside_china, draw a line plot of # cum_cases vs. date, colored by country ggplot(confirmed_cases_top7_outside_china, aes(date, cum_cases, color = country)) + geom_line() + ylab(&quot;Cumulative confirmed cases&quot;) . ── Column specification ──────────────────────────────────────────────────────── cols( country = col_character(), date = col_date(format = &#34;&#34;), cum_cases = col_double() ) . Rows: 2,030 Columns: 3 $ country &lt;chr&gt; &#34;Germany&#34;, &#34;Iran&#34;, &#34;Italy&#34;, &#34;Korea, South&#34;, &#34;Spain&#34;, &#34;US&#34;, &#34;… $ date &lt;date&gt; 2020-02-18, 2020-02-18, 2020-02-18, 2020-02-18, 2020-02-18,… $ cum_cases &lt;dbl&gt; 16, 0, 3, 31, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,… .",
            "url": "https://vosaul.github.io/fastapages-ml/2022/02/05/Visualizing-COVID-19.html",
            "relUrl": "/2022/02/05/Visualizing-COVID-19.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "1. Loading the NIPS papers",
            "content": "# -- YOUR CODE HERE -- import pandas as pd # Read datasets/papers.csv into papers papers = pd.read_csv(&#39;datasets/papers.csv&#39;) # Print out the first rows of papers print(papers.head()) . id year title event_type 0 1 1987 Self-Organization of Associative Database and ... NaN 1 10 1987 A Mean Field Theory of Layer IV of Visual Cort... NaN 2 100 1988 Storing Covariance by the Associative Long-Ter... NaN 3 1000 1994 Bayesian Query Construction for Neural Network... NaN 4 1001 1994 Neural Network Ensembles, Cross Validation, an... NaN pdf_name abstract 0 1-self-organization-of-associative-database-an... Abstract Missing 1 10-a-mean-field-theory-of-layer-iv-of-visual-c... Abstract Missing 2 100-storing-covariance-by-the-associative-long... Abstract Missing 3 1000-bayesian-query-construction-for-neural-ne... Abstract Missing 4 1001-neural-network-ensembles-cross-validation... Abstract Missing paper_text 0 767 n nSELF-ORGANIZATION OF ASSOCIATIVE DATABA... 1 683 n nA MEAN FIELD THEORY OF LAYER IV OF VISU... 2 394 n nSTORING COVARIANCE BY THE ASSOCIATIVE n... 3 Bayesian Query Construction for Neural nNetwor... 4 Neural Network Ensembles, Cross nValidation, a... . 2. Preparing the data for analysis . For the analysis of the papers, we are only interested in the text data associated with the paper as well as the year the paper was published in. . We will analyze this text data using natural language processing. Since the file contains some metadata such as id&#39;s and filenames, it is necessary to remove all the columns that do not contain useful text information. . papers.drop([&#39;id&#39;, &#39;event_type&#39;, &#39;pdf_name&#39;], axis=1, inplace=True) # Print out the first rows of papers print(papers.head()) . year title abstract 0 1987 Self-Organization of Associative Database and ... Abstract Missing 1 1987 A Mean Field Theory of Layer IV of Visual Cort... Abstract Missing 2 1988 Storing Covariance by the Associative Long-Ter... Abstract Missing 3 1994 Bayesian Query Construction for Neural Network... Abstract Missing 4 1994 Neural Network Ensembles, Cross Validation, an... Abstract Missing paper_text 0 767 n nSELF-ORGANIZATION OF ASSOCIATIVE DATABA... 1 683 n nA MEAN FIELD THEORY OF LAYER IV OF VISU... 2 394 n nSTORING COVARIANCE BY THE ASSOCIATIVE n... 3 Bayesian Query Construction for Neural nNetwor... 4 Neural Network Ensembles, Cross nValidation, a... . 3. Plotting how machine learning has evolved over time . In order to understand how the machine learning field has recently exploded in popularity, we will begin by visualizing the number of publications per year. . By looking at the number of published papers per year, we can understand the extent of the machine learning &#39;revolution&#39;! Typically, this significant increase in popularity is attributed to the large amounts of compute power, data and improvements in algorithms. . groups = papers.groupby(&#39;year&#39;) # Determine the size of each group counts = groups.size() # Visualise the counts as a bar plot import matplotlib.pyplot %matplotlib inline counts.plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f45883c8e48&gt; . 4. Preprocessing the text data . Let&#39;s now analyze the titles of the different papers to identify machine learning trends. First, we will perform some simple preprocessing on the titles in order to make them more amenable for analysis. We will use a regular expression to remove any punctuation in the title. Then we will perform lowercasing. We&#39;ll then print the titles of the first rows before and after applying the modification. . import re # Print the titles of the first rows print(papers[&#39;title&#39;].head()) # The following line papers[&#39;title_processed&#39;] = papers[&#39;title&#39;].map(lambda x: re.sub(&#39;[, .!?]&#39;, &#39;&#39;, x)) # Convert the titles to lowercase papers[&#39;title_processed&#39;] = papers[&#39;title_processed&#39;].map(lambda x: x.lower()) # Print the processed titles of the first rows papers[&#39;title_processed&#39;].head() . 0 Self-Organization of Associative Database and ... 1 A Mean Field Theory of Layer IV of Visual Cort... 2 Storing Covariance by the Associative Long-Ter... 3 Bayesian Query Construction for Neural Network... 4 Neural Network Ensembles, Cross Validation, an... Name: title, dtype: object . 0 self-organization of associative database and ... 1 a mean field theory of layer iv of visual cort... 2 storing covariance by the associative long-ter... 3 bayesian query construction for neural network... 4 neural network ensembles cross validation and ... Name: title_processed, dtype: object . 5. A word cloud to visualize the preprocessed text data . In order to verify whether the preprocessing happened correctly, we can make a word cloud of the titles of the research papers. This will give us a visual representation of the most common words. Visualisation is key to understanding whether we are still on the right track! In addition, it allows us to verify whether we need additional preprocessing before further analyzing the text data. . Python has a massive number of open libraries! Instead of trying to develop a method to create word clouds ourselves, we&#39;ll use Andreas Mueller&#39;s wordcloud library. . import wordcloud # Join the different processed titles together. long_string = &#39; &#39;.join(papers[&#39;title_processed&#39;]) # Create a WordCloud object, generate a wordcloud and visualise it wordcloud = wordcloud.WordCloud() # Generate a word cloud wordcloud.generate(long_string) # Visualize the word cloud wordcloud.to_image() . 6. Prepare the text for LDA analysis . The main text analysis method that we will use is latent Dirichlet allocation (LDA). LDA is able to perform topic detection on large document sets, determining what the main &#39;topics&#39; are in a large unlabeled set of texts. A &#39;topic&#39; is a collection of words that tend to co-occur often. The hypothesis is that LDA might be able to clarify what the different topics in the research titles are. These topics can then be used as a starting point for further analysis. . LDA does not work directly on text data. First, it is necessary to convert the documents into a simple vector representation. This representation will then be used by LDA to determine the topics. Each entry of a &#39;document vector&#39; will correspond with the number of times a word occurred in the document. In conclusion, we will convert a list of titles into a list of vectors, all with length equal to the vocabulary. For example, &#39;Analyzing machine learning trends with neural networks.&#39; would be transformed into [1, 0, 1, ..., 1, 0]. . We&#39;ll then plot the 10 most common words based on the outcome of this operation (the list of document vectors). As a check, these words should also occur in the word cloud. . from sklearn.feature_extraction.text import CountVectorizer import numpy as np # Helper function def plot_10_most_common_words(count_data, count_vectorizer): import matplotlib.pyplot as plt words = count_vectorizer.get_feature_names() total_counts = np.zeros(len(words)) for t in count_data: total_counts+=t.toarray()[0] count_dict = (zip(words, total_counts)) count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10] words = [w[0] for w in count_dict] counts = [w[1] for w in count_dict] x_pos = np.arange(len(words)) plt.bar(x_pos, counts,align=&#39;center&#39;) plt.xticks(x_pos, words, rotation=90) plt.xlabel(&#39;words&#39;) plt.ylabel(&#39;counts&#39;) plt.title(&#39;10 most common words&#39;) plt.show() # Initialise the count vectorizer with the English stopwords count_vectorizer = CountVectorizer(stop_words=&#39;english&#39;) # Fit and transform the processed titles count_data = count_vectorizer.fit_transform(papers[&#39;title_processed&#39;]) # Visualise the 10 most common words plot_10_most_common_words(count_data, count_vectorizer) . 7. Analysing trends with LDA . Finally, the research titles will be analyzed using LDA. Note that in order to process a new set of documents (e.g. news articles), a similar set of steps will be required to preprocess the data. The flow that was constructed here can thus easily be exported for a new text dataset. . The only parameter we will tweak is the number of topics in the LDA algorithm. Typically, one would calculate the &#39;perplexity&#39; metric to determine which number of topics is best and iterate over different amounts of topics until the lowest &#39;perplexity&#39; is found. For now, let&#39;s play around with a different number of topics. From there, we can distinguish what each topic is about (&#39;neural networks&#39;, &#39;reinforcement learning&#39;, &#39;kernel methods&#39;, &#39;gaussian processes&#39;, etc.). . import warnings warnings.simplefilter(&quot;ignore&quot;, DeprecationWarning) # Load the LDA model from sk-learn from sklearn.decomposition import LatentDirichletAllocation as LDA # Helper function def print_topics(model, count_vectorizer, n_top_words): words = count_vectorizer.get_feature_names() for topic_idx, topic in enumerate(model.components_): print(&quot; nTopic #%d:&quot; % topic_idx) print(&quot; &quot;.join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]])) # Tweak the two parameters below (use int values below 15) number_topics = 10 number_words = 10 # Create and fit the LDA model lda = LDA(n_components=number_topics) lda.fit(count_data) # Print the topics found by the LDA model print(&quot;Topics found via LDA:&quot;) print_topics(lda, count_vectorizer, number_words) . Topics found via LDA: Topic #0: information linear structured representations algorithm robust minimization learning brain distributed Topic #1: models bayesian sparse estimation learning random sampling latent graphical networks Topic #2: optimization gradient learning matrix multiple approach object descent state dynamics Topic #3: multi large learning functions selection scale neural decision training boosting Topic #4: learning hierarchical unsupervised memory rank order convex low model carlo Topic #5: learning analysis efficient online reinforcement bounds systems neurons kernels data Topic #6: stochastic time optimal neural fast learning networks high dimensional model Topic #7: learning based gaussian inference models classification processes process probabilistic variational Topic #8: neural networks clustering data deep image visual recognition feature model Topic #9: learning supervised detection human semi images natural kernel reduction free . 8. The future of machine learning . Machine learning has become increasingly popular over the past years. The number of NIPS conference papers has risen exponentially, and people are continuously looking for ways on how they can incorporate machine learning into their products and services. . Although this analysis focused on analyzing machine learning trends in research, a lot of these techniques are rapidly being adopted in industry. Following the latest machine learning trends is a critical skill for a data scientist, and it is recommended to continuously keep learning by going through blogs, tutorials, and courses. . more_papers_published_in_2018 = ... .",
            "url": "https://vosaul.github.io/fastapages-ml/2022/02/05/The-Hottest-Topics-in-Machine-Learning.html",
            "relUrl": "/2022/02/05/The-Hottest-Topics-in-Machine-Learning.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Bitcoin and Cryptocurrencies",
            "content": "1. Bitcoin and Cryptocurrencies: Full dataset, filtering, and reproducibility . Since the launch of Bitcoin in 2008, hundreds of similar projects based on the blockchain technology have emerged. We call these cryptocurrencies (also coins or cryptos in the Internet slang). Some are extremely valuable nowadays, and others may have the potential to become extremely valuable in the future1. In fact, on the 6th of December of 2017, Bitcoin has a market capitalization above $200 billion. . The astonishing increase of Bitcoin market capitalization in 2017. . *1 WARNING: The cryptocurrency market is exceptionally volatile2 and any money you put in might disappear into thin air. Cryptocurrencies mentioned here might be scams similar to Ponzi Schemes or have many other issues (overvaluation, technical, etc.). Please do not mistake this for investment advice. * . 2 Update on March 2020: Well, it turned out to be volatile indeed :D . That said, let&#39;s get to business. We will start with a CSV we conveniently downloaded on the 6th of December of 2017 using the coinmarketcap API (NOTE: The public API went private in 2020 and is no longer available) named datasets/coinmarketcap_06122017.csv. . Importing pandas . import pandas as pd . Importing matplotlib and setting aesthetics for plotting later. . import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format = &#39;svg&#39; plt.style.use(&#39;fivethirtyeight&#39;) . Reading datasets/coinmarketcap_06122017.csv into pandas . dec6 = pd.read_csv(&#39;datasets/coinmarketcap_06122017.csv&#39;) . Selecting the &#39;id&#39; and the &#39;market_cap_usd&#39; columns . market_cap_raw = dec6[[&#39;id&#39;, &#39;market_cap_usd&#39;]] . Counting the number of values . market_cap_raw.count() . 2. Discard the cryptocurrencies without a market capitalization . Why do the count() for id and market_cap_usd differ above? It is because some cryptocurrencies listed in coinmarketcap.com have no known market capitalization, this is represented by NaN in the data, and NaNs are not counted by count(). These cryptocurrencies are of little interest to us in this analysis, so they are safe to remove. . cap = market_cap_raw.query(&#39;market_cap_usd &gt; 0&#39;) # Counting the number of values again cap.count() . id 1031 market_cap_usd 1031 dtype: int64 . 3. How big is Bitcoin compared with the rest of the cryptocurrencies? . At the time of writing, Bitcoin is under serious competition from other projects, but it is still dominant in market capitalization. Let&#39;s plot the market capitalization for the top 10 coins as a barplot to better visualize this. . TOP_CAP_TITLE = &#39;Top 10 market capitalization&#39; TOP_CAP_YLABEL = &#39;% of total cap&#39; # Selecting the first 10 rows and setting the index cap10 = cap[:10].set_index(&#39;id&#39;) # Calculating market_cap_perc cap10 = cap10.assign(market_cap_perc = lambda x: (x.market_cap_usd / cap.market_cap_usd.sum())*100) # Plotting the barplot with the title defined above ax = cap10.market_cap_perc.plot.bar(title=TOP_CAP_TITLE) # Annotating the y axis with the label defined above ax.set_ylabel(TOP_CAP_YLABEL); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 4. Making the plot easier to read and more informative . While the plot above is informative enough, it can be improved. Bitcoin is too big, and the other coins are hard to distinguish because of this. Instead of the percentage, let&#39;s use a log10 scale of the &quot;raw&quot; capitalization. Plus, let&#39;s use color to group similar coins and make the plot more informative1. . For the colors rationale: bitcoin-cash and bitcoin-gold are forks of the bitcoin blockchain2. Ethereum and Cardano both offer Turing Complete smart contracts. Iota and Ripple are not minable. Dash, Litecoin, and Monero get their own color. . 1 This coloring is a simplification. There are more differences and similarities that are not being represented here. . 2 The bitcoin forks are actually very different, but it is out of scope to talk about them here. Please see the warning above and do your own research. . COLORS = [&#39;orange&#39;, &#39;green&#39;, &#39;orange&#39;, &#39;cyan&#39;, &#39;cyan&#39;, &#39;blue&#39;, &#39;silver&#39;, &#39;orange&#39;, &#39;red&#39;, &#39;green&#39;] # Plotting market_cap_usd as before but adding the colors and scaling the y-axis ax = cap10.market_cap_usd.plot.bar(title=TOP_CAP_TITLE, logy=True, color = COLORS) # Annotating the y axis with log(USD) ax.set_ylabel(&#39;USD&#39;) # Final touch! Removing the xlabel as it is not very informative ax.set_xlabel(&#39;&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 5. What is going on?! Volatility in cryptocurrencies . The cryptocurrencies market has been spectacularly volatile since the first exchange opened. This notebook didn&#39;t start with a big, bold warning for nothing. Let&#39;s explore this volatility a bit more! We will begin by selecting and plotting the 24 hours and 7 days percentage change, which we already have available. . volatility = dec6[[&#39;id&#39;, &#39;percent_change_24h&#39;, &#39;percent_change_7d&#39;]] # Setting the index to &#39;id&#39; and dropping all NaN rows volatility = volatility.set_index(&#39;id&#39;).dropna() # Sorting the DataFrame by percent_change_24h in ascending order volatility = volatility.sort_values(&#39;percent_change_24h&#39;) # Checking the first few rows volatility.head() . percent_change_24h percent_change_7d . id . flappycoin -95.85 | -96.61 | . credence-coin -94.22 | -95.31 | . coupecoin -93.93 | -61.24 | . tyrocoin -79.02 | -87.43 | . petrodollar -76.55 | 542.96 | . 6. Well, we can already see that things are a bit crazy . It seems you can lose a lot of money quickly on cryptocurrencies. Let&#39;s plot the top 10 biggest gainers and top 10 losers in market capitalization. . def top10_subplot(volatility_series, title): # making the subplot and the figure for nrows and ncolumns fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6)) # Plotting with pandas the barchart for the top 10 losers with the color RED ax = volatility_series[:10].plot.bar(color=&quot;darkred&quot;, ax=axes[0]) # Setting the main title to TITLE fig.suptitle(title) # Setting the ylabel to &quot;% change&quot; ax.set_ylabel(&#39;% change&#39;) # Same as above, but for the top 10 winners and in darkblue ax = volatility_series[-10:].plot.bar(color=&quot;darkblue&quot;, ax=axes[1]) # Returning this for good practice, might use later return fig, ax DTITLE = &quot;24 hours top losers and winners&quot; # Calling the function above with the volatility.percent_change_24h series # and title DTITLE fig, ax = top10_subplot(volatility.percent_change_24h, DTITLE) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 7. Ok, those are... interesting. Let&#39;s check the weekly Series too. . 800% daily increase?! Why are we doing this tutorial and not buying random coins?1 . After calming down, let&#39;s reuse the function defined above to see what is going weekly instead of daily. . 1 Please take a moment to understand the implications of the red plots on how much value some cryptocurrencies lose in such short periods of time . volatility7d = volatility.sort_values(&quot;percent_change_7d&quot;) WTITLE = &quot;Weekly top losers and winners&quot; # Calling the top10_subplot function fig, ax = top10_subplot(volatility7d.percent_change_7d, WTITLE); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 8. How small is small? . The names of the cryptocurrencies above are quite unknown, and there is a considerable fluctuation between the 1 and 7 days percentage changes. As with stocks, and many other financial products, the smaller the capitalization, the bigger the risk and reward. Smaller cryptocurrencies are less stable projects in general, and therefore even riskier investments than the bigger ones1. Let&#39;s classify our dataset based on Investopedia&#39;s capitalization definitions for company stocks. . 1 Cryptocurrencies are a new asset class, so they are not directly comparable to stocks. Furthermore, there are no limits set in stone for what a &quot;small&quot; or &quot;large&quot; stock is. Finally, some investors argue that bitcoin is similar to gold, this would make them more comparable to a commodity instead. . largecaps = cap.query(&quot;market_cap_usd &gt; 1E+10&quot;) # Printing out largecaps largecaps . id market_cap_usd . 0 bitcoin | 2.130493e+11 | . 1 ethereum | 4.352945e+10 | . 2 bitcoin-cash | 2.529585e+10 | . 3 iota | 1.475225e+10 | . 9. Most coins are tiny . Note that many coins are not comparable to large companies in market cap, so let&#39;s divert from the original Investopedia definition by merging categories. . This is all for now. Thanks for completing this project! . # &quot;cap&quot; DataFrame. Returns an int. # INSTRUCTORS NOTE: Since you made it to the end, consider it a gift :D def capcount(query_string): return cap.query(query_string).count().id # Labels for the plot LABELS = [&quot;biggish&quot;, &quot;micro&quot;, &quot;nano&quot;] # Using capcount count the not_so_small cryptos biggish = capcount(&quot;market_cap_usd &gt; 3E+8&quot;) # Same as above for micro ... micro = capcount(&quot;market_cap_usd &gt;= 5E+7 &amp; market_cap_usd &lt; 3E+8&quot;) # ... and for nano nano = capcount(&quot;market_cap_usd &lt; 5E+7&quot;) # Making a list with the 3 counts values = [biggish, micro, nano] # Plotting them with matplotlib plt.bar(range(len(values)), values, tick_label=LABELS); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://vosaul.github.io/fastapages-ml/2022/02/05/Exploring-the-Bitcoin-Crptocurrency-Market.html",
            "relUrl": "/2022/02/05/Exploring-the-Bitcoin-Crptocurrency-Market.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://vosaul.github.io/fastapages-ml/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://vosaul.github.io/fastapages-ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vosaul.github.io/fastapages-ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}